{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64940477",
   "metadata": {},
   "source": [
    "# Predicting Bitcoin Prices via Mathematical and Financial Models: A Study of the Time-Series Analysis of the economic and macroeconomic factors and of the technical indicators and sentiment analysis\n",
    "\n",
    "Boulouma A., 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b2aab",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "The problem is to develop a predictive model for the bitcoin market prices that can accurately forecast the price at time $t+1$ based on the price at time $t$.\n",
    "\n",
    "### Research Question\n",
    "\n",
    "What is the best predictive model to use for forecasting bitcoin market prices, and what is the predictive power of each model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d020bde",
   "metadata": {},
   "source": [
    "## Model 2 - VARMA-NN: A Hybrid Model for Multivariate Time Series Forecasting Using Neural Networks and Vector Autoregressive Moving Average Models\n",
    "\n",
    "Time series forecasting is an essential problem in several fields such as finance, economics, and engineering, and has been an active research topic for several decades. Among the existing forecasting models, the VARMA-NN model is a more powerful and flexible extension of the ARIMA-NN model, as it can handle multivariate time series data and capture both linear and non-linear dependencies between variables. The VARMA-NN model combines the VARMA model with a neural network model, similar to the ARIMA-NN model. This paper discusses the VARMA-NN model proposed by Zhang, G. P. (2003) in detail and its implementation to forecast economic and macroeconomic indicators, technical indicators, and sentiment analysis for predicting future values.\n",
    "\n",
    "The VARMA model is a generalization of the VAR model, where it includes the moving average terms in addition to the autoregressive terms. The VARMA model is specified by two parameters: the order of the autoregressive terms $p$ and the order of the moving average terms $q$. The VARMA model can be written as:\n",
    "\n",
    "$$\\sum_{i=1}^{p} \\Phi_i L^i \\Delta Y_t = \\sum_{j=1}^{q} \\Theta_j L^j \\epsilon_t$$\n",
    "\n",
    "where $\\Delta Y_t = (Y_t - Y_{t-1})$ is the differenced time series, $L$ is the lag operator, $\\Phi_i$ and $\\Theta_j$ are the autoregressive and moving average coefficient matrices, respectively, and $\\epsilon_t$ is the white noise error term at time $t$.\n",
    "\n",
    "The VARMA-NN model combines the VARMA model with a neural network model in a similar way to the ARIMA-NN model. The VARMA-NN model can be formalized as follows:\n",
    "\n",
    "1. Preprocessing: The time series data is preprocessed to remove any outliers or missing values. The data is then split into training and testing sets.\n",
    "\n",
    "2. VARMA modeling: The VARMA model is fit to the training data. The VARMA model is specified by two parameters: $p$ and $q$. The VARMA model captures the linear dependencies between past and future values of the multivariate time series.\n",
    "\n",
    "3. Neural network modeling: A neural network model is built using the training data. The neural network can be a feedforward neural network or a recurrent neural network (RNN). The neural network captures the non-linear dependencies that may exist in the data.\n",
    "\n",
    "4. Hybrid modeling: The predictions from the VARMA model and the neural network model are combined using a weighted average. The weights are determined by the relative performance of the two models on the training data.\n",
    "\n",
    "5. Evaluation: The performance of the VARMA-NN model is evaluated using the testing data. The evaluation metrics used can include mean squared error (MSE), root mean squared error (RMSE), and mean absolute percentage error (MAPE).\n",
    "\n",
    "6. Interpretation: Interpret the model results and use them to inform decision-making. It is important to keep in mind that the model results are not a crystal ball and should be used in conjunction with other factors and expert judgment.\n",
    "\n",
    "7. Updating the model: As new data becomes available, the model should be updated and refined to ensure it remains accurate and relevant.\n",
    "\n",
    "The equations used in the VARMA-NN model can be represented as:\n",
    "\n",
    "### VARMA model:\n",
    "\n",
    "$$\\sum_{i=1}^{p} \\Phi_i L^i \\Delta Y_t = \\sum_{j=1}^{q} \\Theta_j L^j \\epsilon_t$$\n",
    "\n",
    "where $\\Delta Y_t$ is the differenced multivariate time series, $\\Phi_i$ and $\\Theta_j$ are the autoregressive and moving average coefficient matrices, respectively, and $\\epsilon_t$ is the white noise error term at time $t$.\n",
    "\n",
    "### Neural network model:\n",
    "\n",
    "$$Y_t = f(WX_t + b)$$\n",
    "\n",
    "where $Y_t$ is the predicted value at time $t$, $X_t$ is the input vector at time $t$, $W$ is the weight matrix, $b$ is the bias vector, and $f$ is the activation function.\n",
    "\n",
    "### Hybrid model:\n",
    "\n",
    "$$Y_t = \\alpha Y_t^{VARMA} + (1 - \\alpha) Y_t^{NN}$$\n",
    "\n",
    "where $Y_t^{ARIMA}$ is the prediction from the ARIMA model, $Y_t^{NN}$ is the prediction from the neural network model, and $\\alpha$ is the weight assigned to the ARIMA prediction.\n",
    "\n",
    "The specific formulas for each indicator are as follows:\n",
    "\n",
    "The moving average indicator ($MA$): The formula for $MA$ with a window size of $k$ can be written as:\n",
    "$$Y_{MA,t} = \\frac{1}{k} \\sum_{i=t-k+1}^{t} X_i$$\n",
    "\n",
    "where $X_i$ is the value of the variable at time $i$.\n",
    "\n",
    "The relative strength index ($RSI$): The formula for $RSI$ with a window size of $k$ can be written as:\n",
    "$$Y_{RSI,t} = 100 - \\frac{100}{1 + RS}$$\n",
    "\n",
    "where $RS$ is the relative strength at time $t$, which is calculated as:\n",
    "\n",
    "$$RS = \\frac{\\sum_{i=t-k+1}^{t} Max(X_i - X_{i-1}, 0)}{\\sum_{i=t-k+1}^{t} |X_i - X_{i-1}|}$$\n",
    "\n",
    "The stochastic oscillator ($SO$): The formula for $SO$ with a window size of $k$ can be written as:\n",
    "$$Y_{SO,t} = \\frac{X_t - Min_{k}(X)}{Max_{k}(X) - Min_{k}(X)} \\times 100$$\n",
    "\n",
    "where $Min_{k}(X)$ and $Max_{k}(X)$ are the minimum and maximum values of the variable over the past $k$ periods, respectively.\n",
    "\n",
    "The Google Trend indicator $f_{GT}(Q_t)$: The formula for $f_{GT}(Q_t)$ is:\n",
    "$$Y_{GT,t} = f_{GT}(Q_t)$$\n",
    "\n",
    "where $Q_t$ represents the search query related to Bitcoin at time $t$, and $f_{GT}$ is a function that processes the search data to generate a Google Trends score.\n",
    "\n",
    "Overall, the VARMA-NN model offers a powerful and flexible approach to time series forecasting, particularly for multivariate data with both linear and non-linear dependencies. It combines the strengths of both the VARMA and neural network models, allowing it to capture complex relationships between variables.\n",
    "\n",
    "However, it is important to note that the model is not a one-size-fits-all solution and must be tailored to the specific data and problem at hand. It also requires a significant amount of data and computational resources to train and optimize, so it may not be suitable for all applications.\n",
    "\n",
    "Nonetheless, the VARMA-NN model represents a significant advancement in time series forecasting and has the potential to greatly improve our ability to predict future trends and outcomes. Further research and development in this area are likely to yield even more powerful and effective forecasting methods in the future.\n",
    "\n",
    "### Seasonal decomposition of time series (STL) \n",
    "\n",
    "- Zhang, G. P. (2003). Time series forecasting using a hybrid ARIMA and neural network model. Neurocomputing, 50, 159-175.\n",
    "\n",
    "- Box, G. E. P., & Jenkins, G. M. (1976). Time series analysis: Forecasting and control. San Francisco, CA: Holden-Day.\n",
    "\n",
    "- Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: Principles and practice. OTexts.\n",
    "\n",
    "- Chollet, F. (2018). Deep learning with Python. Manning Publications.\n",
    "\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5884cd",
   "metadata": {},
   "source": [
    "## Build a new dataset based on the model and time entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "a7d74a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the function to generate new data\n",
    "def generate_new_data(model, time_input):\n",
    "    with open('models/best_ltsm_model.pkl', 'rb') as file:\n",
    "        n_features = len(df.columns)\n",
    "        model = pickle.load(file)\n",
    "        # Convert the time input to the same format as the timestamps used in the training data\n",
    "        time_input = pd.Timestamp(time_input)\n",
    "\n",
    "        # Create a numpy array with the same shape as the input data used during training\n",
    "        input_data = np.empty((1, n_steps, n_features))\n",
    "        input_data[:] = np.nan\n",
    "\n",
    "        # Replace the values in the numpy array for the timestamp corresponding to the time input with 0\n",
    "        index = df.index.get_loc(time_input)\n",
    "        input_data[0, -n_steps+index:, :] = 0\n",
    "\n",
    "        # Use the loaded model to make a prediction on the numpy array\n",
    "        predictions_scaled = model.predict(input_data).flatten()\n",
    "\n",
    "        # Convert the predicted values back to their original scale\n",
    "        predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, n_features))[0]\n",
    "\n",
    "        # Return the predicted values for each feature\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "69628e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-02-28 15:50:00         1797\n",
      "metadata.json                                  2023-02-28 15:50:00           64\n",
      "variables.h5                                   2023-02-28 15:50:00       160928\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-02-28 15:50:00         1797\n",
      "metadata.json                                  2023-02-28 15:50:00           64\n",
      "variables.h5                                   2023-02-28 15:50:00       160928\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "1/1 [==============================] - 0s 251ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_data_df = pd.DataFrame()\n",
    "\n",
    "with open('models/best_ltsm_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    new_data = generate_new_data(model, '2022-03-01')\n",
    "    new_data_df = pd.DataFrame([new_data], columns=df.columns)\n",
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "2a72c923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>hash_rate</th>\n",
       "      <th>transaction_volume</th>\n",
       "      <th>mining_difficulty</th>\n",
       "      <th>inflation_rate</th>\n",
       "      <th>bitcoin_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price  hash_rate  transaction_volume  mining_difficulty  inflation_rate  \\\n",
       "0    NaN        NaN                 NaN                NaN             NaN   \n",
       "\n",
       "   bitcoin_trend  \n",
       "0            NaN  "
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27bb2d",
   "metadata": {},
   "source": [
    "### 1.2. VARMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "9fe5e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/m0hv5jzd1kn2nrn5y5w241g00000gn/T/ipykernel_8750/3953675569.py:7: DtypeWarning: Columns (146) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  df = btc_df = clean_and_transform_data(read_data(\"datasets/btc.csv\"), read_data(\"datasets/btc_google_trend.csv\"))\n",
      "/Users/macbook/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/statespace/varmax.py:161: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.\n",
      "  warn('Estimation of VARMA(p,q) models is not generically robust,'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "exog is not 1d or 2d",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gc/m0hv5jzd1kn2nrn5y5w241g00000gn/T/ipykernel_8750/3953675569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train the VARMAX model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVARMAX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/statespace/varmax.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, order, trend, error_cov_type, measurement_error, enforce_stationarity, enforce_invertibility, trend_offset, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Initialize the state space model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         super(VARMAX, self).__init__(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_posdef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_posdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, k_states, exog, dates, freq, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m                  **kwargs):\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Initialize the model base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         super(MLEModel, self).__init__(endog=endog, exog=exog,\n\u001b[0m\u001b[1;32m    137\u001b[0m                                        \u001b[0mdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                                        missing='none')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, dates, freq, missing, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m     def __init__(self, endog, exog=None, dates=None, freq=None,\n\u001b[1;32m    410\u001b[0m                  missing='none', **kwargs):\n\u001b[0;32m--> 411\u001b[0;31m         super(TimeSeriesModel, self).__init__(endog, exog, missing=missing,\n\u001b[0m\u001b[1;32m    412\u001b[0m                                               **kwargs)\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0m\u001b[1;32m     78\u001b[0m                                       **kwargs)\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0m\u001b[1;32m    673\u001b[0m                  **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mxarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exog is not 1d or 2d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0myarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxarr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: exog is not 1d or 2d"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "df = btc_df = clean_and_transform_data(read_data(\"datasets/btc.csv\"), read_data(\"datasets/btc_google_trend.csv\"))\n",
    "\n",
    "# Set the date column as the index\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Define the number of time steps for the input data\n",
    "n_steps = 3\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size,:]\n",
    "test_data = scaled_data[train_size-n_steps:]\n",
    "\n",
    "# Define a function to prepare the input and output data for the model\n",
    "def prepare_data(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data[i-n_steps:i,:])\n",
    "        y.append(data[i,:])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = prepare_data(train_data, n_steps)\n",
    "test_X, test_y = prepare_data(test_data, n_steps)\n",
    "\n",
    "# Train the VARMAX model\n",
    "model = VARMAX(endog=train_y, exog=train_X, order=(1,1))\n",
    "result = model.fit()\n",
    "\n",
    "# Define a function to make predictions for any variable\n",
    "def predict_variable(variable, time):\n",
    "    # Prepare the input data\n",
    "    input_data = df.loc[time-np.timedelta64(n_steps-1, 'D'):time,:]\n",
    "    input_data_scaled = scaler.transform(input_data)\n",
    "    input_data_reshaped = input_data_scaled.reshape(1, n_steps, train_X.shape[2])\n",
    "    input_data_reshaped = input_data_reshaped[:, :, :-1]\n",
    "\n",
    "    # Make predictions for the variable\n",
    "    predictions_scaled = result.forecast(exog=input_data_reshaped, steps=n_steps)\n",
    "    predictions = scaler.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    return predictions[0][df.columns.get_loc(variable)]\n",
    "\n",
    "# Example usage:\n",
    "predicted_price = predict_variable(\"Price\", pd.Timestamp(\"2022-01-01\"))\n",
    "print(f\"Predicted BTC price on 2022-01-01: {predicted_price:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
